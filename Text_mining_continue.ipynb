{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2093eb-922b-4be6-ad55-668abf563f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb03113d-b8a6-4bf3-8c57-ee195f771628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5347b766-b38f-4a13-a98f-e33238f5e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.3.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.9 MB 524.3 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.5/13.9 MB 524.3 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 0.8/13.9 MB 588.4 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.8/13.9 MB 588.4 kB/s eta 0:00:23\n",
      "   --- ------------------------------------ 1.0/13.9 MB 653.7 kB/s eta 0:00:20\n",
      "   --- ------------------------------------ 1.3/13.9 MB 721.7 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.6/13.9 MB 814.1 kB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 1.8/13.9 MB 875.3 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 2.4/13.9 MB 979.5 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.6/13.9 MB 1.0 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 3.1/13.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.7/13.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 3.9/13.9 MB 1.3 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.5/13.9 MB 1.3 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 5.0/13.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.2/13.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.8/13.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.3/13.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.8/13.9 MB 1.6 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 7.3/13.9 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.9/13.9 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 8.7/13.9 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 9.2/13.9 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.7/13.9 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.2/13.9 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 10.7/13.9 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.0/13.9 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.5/13.9 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.8/13.9 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.8/13.9 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.1/13.9 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.1/13.9 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/13.9 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/13.9 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/13.9 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/13.9 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.1/13.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.1/13.9 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/13.9 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/13.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.6/13.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.6/13.9 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl (116 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.6 kB ? eta -:--:--\n",
      "   -------------------------------------- 632.6/632.6 kB 719.3 kB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/1.7 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.7 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.2 MB/s eta 0:00:00\n",
      "Using cached numpy-2.3.1-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.3.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.0/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.3/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.1/6.3 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.4/6.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 2.9/6.3 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.1/6.3 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.4/6.3 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 3.9/6.3 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.2/6.3 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.0/6.3 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 1.8 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.3/5.4 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.1/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.9/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 3.7/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, numpy, murmurhash, marisa-trie, cloudpathlib, catalogue, srsly, preshed, language-data, blis, typer, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.13 numpy-2.3.1 preshed-3.0.10 shellingham-1.5.4 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 wasabi-1.1.3 weasel-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.1 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.1 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7806289d-0816-4f1a-b7ea-51db18d7b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_mining.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c462ad4c-7a33-42a2-a9c0-633188eab6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) is a branch of computer science that aims to create machines capable of performing tasks that typically require human intelligence. These tasks include learning from experience, understanding natural language, recognizing images and patterns, and making decisions. AI is already integrated into many aspects of our daily lives, such as virtual assistants, recommendation systems, facial recognition, and self-driving cars. It uses technologies like machine learning, deep learning, and natural language processing to analyze large amounts of data and improve its performance over time. As AI continues to evolve, it holds the potential to greatly transform industries such as healthcare, education, transportation, and finance, while also raising important questions about ethics, privacy, and job displacement.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ccfe07-bb78-4e38-bacc-4a5eb19e66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file open garne  arko way bhaneko pathlib bataa ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46de6ce9-d5e8-4450-8d00-5464e683b582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathlib\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pathlib\n",
      "Successfully installed pathlib-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pathlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40eb6dbb-9cdf-41b3-ba4b-bb648e829c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a5e5e8d-feda-4e5f-812f-cc989e92affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = Path('text_mining.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "690c95f3-093b-417f-9de4-3df55285bbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) is a branch of computer science that aims to create machines capable of performing tasks that typically require human intelligence. These tasks include learning from experience, understanding natural language, recognizing images and patterns, and making decisions. AI is already integrated into many aspects of our daily lives, such as virtual assistants, recommendation systems, facial recognition, and self-driving cars. It uses technologies like machine learning, deep learning, and natural language processing to analyze large amounts of data and improve its performance over time. As AI continues to evolve, it holds the potential to greatly transform industries such as healthcare, education, transportation, and finance, while also raising important questions about ethics, privacy, and job displacement.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f465a-3f1b-4893-87ac-e539fe377fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850612b-260e-4b22-81a8-701272f70f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127f62e-6f1a-46e4-b2e5-59867bb74312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845dbc41-a546-47cd-8144-25022e1308a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe1d8f-f695-4b7d-98d9-df698d6dfc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "natural labguage tool kit => nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4035d-2dba-4e92-98b5-7424d5ed9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "173d2788-1a06-420a-9b39-06b8ea68ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4203abe1-6041-40be-b1e4-1eec5107466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b003986-ffdb-411e-a17a-8c4e632cac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'a', 'branch', 'of', 'computer', 'science', 'that', 'aims', 'to', 'create', 'machines', 'capable', 'of', 'performing', 'tasks', 'that', 'typically', 'require', 'human', 'intelligence', '.', 'These', 'tasks', 'include', 'learning', 'from', 'experience', ',', 'understanding', 'natural', 'language', ',', 'recognizing', 'images', 'and', 'patterns', ',', 'and', 'making', 'decisions', '.', 'AI', 'is', 'already', 'integrated', 'into', 'many', 'aspects', 'of', 'our', 'daily', 'lives', ',', 'such', 'as', 'virtual', 'assistants', ',', 'recommendation', 'systems', ',', 'facial', 'recognition', ',', 'and', 'self-driving', 'cars', '.', 'It', 'uses', 'technologies', 'like', 'machine', 'learning', ',', 'deep', 'learning', ',', 'and', 'natural', 'language', 'processing', 'to', 'analyze', 'large', 'amounts', 'of', 'data', 'and', 'improve', 'its', 'performance', 'over', 'time', '.', 'As', 'AI', 'continues', 'to', 'evolve', ',', 'it', 'holds', 'the', 'potential', 'to', 'greatly', 'transform', 'industries', 'such', 'as', 'healthcare', ',', 'education', ',', 'transportation', ',', 'and', 'finance', ',', 'while', 'also', 'raising', 'important', 'questions', 'about', 'ethics', ',', 'privacy', ',', 'and', 'job', 'displacement', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(content)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205f4ef-91cc-43c6-91ac-aee092afba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens ma chhutaunu bhaneko paragraph lai individual unit ie: word ma chhhutaunu ho. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d5ecf19-4379-49fb-9df7-3a767bcf6049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(AI)',\n",
       " 'is',\n",
       " 'a',\n",
       " 'branch',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'that',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'create',\n",
       " 'machines',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'performing',\n",
       " 'tasks',\n",
       " 'that',\n",
       " 'typically',\n",
       " 'require',\n",
       " 'human',\n",
       " 'intelligence.',\n",
       " 'These',\n",
       " 'tasks',\n",
       " 'include',\n",
       " 'learning',\n",
       " 'from',\n",
       " 'experience,',\n",
       " 'understanding',\n",
       " 'natural',\n",
       " 'language,',\n",
       " 'recognizing',\n",
       " 'images',\n",
       " 'and',\n",
       " 'patterns,',\n",
       " 'and',\n",
       " 'making',\n",
       " 'decisions.',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'already',\n",
       " 'integrated',\n",
       " 'into',\n",
       " 'many',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'our',\n",
       " 'daily',\n",
       " 'lives,',\n",
       " 'such',\n",
       " 'as',\n",
       " 'virtual',\n",
       " 'assistants,',\n",
       " 'recommendation',\n",
       " 'systems,',\n",
       " 'facial',\n",
       " 'recognition,',\n",
       " 'and',\n",
       " 'self-driving',\n",
       " 'cars.',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'technologies',\n",
       " 'like',\n",
       " 'machine',\n",
       " 'learning,',\n",
       " 'deep',\n",
       " 'learning,',\n",
       " 'and',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'data',\n",
       " 'and',\n",
       " 'improve',\n",
       " 'its',\n",
       " 'performance',\n",
       " 'over',\n",
       " 'time.',\n",
       " 'As',\n",
       " 'AI',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'evolve,',\n",
       " 'it',\n",
       " 'holds',\n",
       " 'the',\n",
       " 'potential',\n",
       " 'to',\n",
       " 'greatly',\n",
       " 'transform',\n",
       " 'industries',\n",
       " 'such',\n",
       " 'as',\n",
       " 'healthcare,',\n",
       " 'education,',\n",
       " 'transportation,',\n",
       " 'and',\n",
       " 'finance,',\n",
       " 'while',\n",
       " 'also',\n",
       " 'raising',\n",
       " 'important',\n",
       " 'questions',\n",
       " 'about',\n",
       " 'ethics,',\n",
       " 'privacy,',\n",
       " 'and',\n",
       " 'job',\n",
       " 'displacement.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yesari pani tokens generate garnaa sakenchhaa . \n",
    "\n",
    "tokens = content.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4390c8-27b7-4409-9cd1-18015d43b9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aec497-cba1-4c62-b631-e1911113fd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3cf19c8-8666-461a-b19f-198a70dc80a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 96 samples and 116 outcomes>\n"
     ]
    }
   ],
   "source": [
    "#Most frequent \n",
    "from nltk.probability import FreqDist\n",
    "frequency = FreqDist(tokens)\n",
    "print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4adf98-2ad6-4d05-b880-5b8670c22490",
   "metadata": {},
   "outputs": [],
   "source": [
    "total 116 word ma uniquye word 96 otaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d9100c6-a2ea-4c1d-a26b-68bf9888f5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 7),\n",
       " ('of', 4),\n",
       " ('to', 4),\n",
       " ('is', 2),\n",
       " ('that', 2),\n",
       " ('tasks', 2),\n",
       " ('natural', 2),\n",
       " ('AI', 2),\n",
       " ('such', 2),\n",
       " ('as', 2)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency.most_common(10) #top 10 most frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd8c89-0429-4773-951f-077cf102a416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a9eef-3bcd-4f93-8fea-64916c5808ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7195e59-ad7c-4878-92ca-062e862f8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation marks hatauna parchha dherei jasto case ma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76da3e9a-d217-449a-b723-dc79163dd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Punctuation Removal\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cac9d7b-d758-47ea-94c9-ab2621b4cce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation           #punctuation jati sabei rakheko hunchhaaa yehaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b5ae8-0916-4929-90f7-0f834aa3d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "yo punctuation ahru lai replace grnu parnee hunchhaa translator use gareraa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33e69b77-400b-4196-be39-e67ab68dade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('','', string.punctuation) \n",
    "after_content = content.translate(translator)\n",
    "\n",
    "\n",
    "#if the liyeko token chai string.punctuation ma chha bhane teslai chai empty ley represent\n",
    "#'','' yemi harule\n",
    "#1st ko '' le token read garne\n",
    "#arko '' le replace garneee. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04ccc601-0ac1-431d-8063-65b499a82541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence AI is a branch of computer science that aims to create machines capable of performing tasks that typically require human intelligence These tasks include learning from experience understanding natural language recognizing images and patterns and making decisions AI is already integrated into many aspects of our daily lives such as virtual assistants recommendation systems facial recognition and selfdriving cars It uses technologies like machine learning deep learning and natural language processing to analyze large amounts of data and improve its performance over time As AI continues to evolve it holds the potential to greatly transform industries such as healthcare education transportation and finance while also raising important questions about ethics privacy and job displacement'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd468a4-643c-418a-9b50-6f611d4f19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yehaaa kunei pani punctuation chhaina. taraa \\n chai dekhachhaa katei katei it is back slash that means new line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39be86-f5e9-4124-ae96-63362541c7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992f2b4-6945-4f5f-b666-c07633637652",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop words bhaneko chai => a , and , the , of, but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca10984-11b0-4969-a2bf-19ec85dc44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus => collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8f826c9-5f74-4237-8813-ab26e23e7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords removal\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af5a74b3-9184-4d17-97c6-353c10a9f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68587eff-17aa-4a14-84ac-fc477cf03bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "796e625c-7a1a-4c19-af5a-c412a5694a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "633147a7-74fa-4f35-b61f-4ad828ac184e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de3c58-b733-4a60-b9dd-0590868c98d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f5d9c2f-2723-40f3-95a1-4334ac2a3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_stopwords = []\n",
    "for word in tokens:\n",
    "    if word not in stop_words:\n",
    "        after_stopwords.append(word)  #stops word ma navako words haru filter bhayeraaa after_stopwords ma janchha yesto garda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2fc6202-66ce-4476-9757-61d2608b8008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(AI)',\n",
       " 'branch',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'aims',\n",
       " 'create',\n",
       " 'machines',\n",
       " 'capable',\n",
       " 'performing',\n",
       " 'tasks',\n",
       " 'typically',\n",
       " 'require',\n",
       " 'human',\n",
       " 'intelligence.',\n",
       " 'These',\n",
       " 'tasks',\n",
       " 'include',\n",
       " 'learning',\n",
       " 'experience,',\n",
       " 'understanding',\n",
       " 'natural',\n",
       " 'language,',\n",
       " 'recognizing',\n",
       " 'images',\n",
       " 'patterns,',\n",
       " 'making',\n",
       " 'decisions.',\n",
       " 'AI',\n",
       " 'already',\n",
       " 'integrated',\n",
       " 'many',\n",
       " 'aspects',\n",
       " 'daily',\n",
       " 'lives,',\n",
       " 'virtual',\n",
       " 'assistants,',\n",
       " 'recommendation',\n",
       " 'systems,',\n",
       " 'facial',\n",
       " 'recognition,',\n",
       " 'self-driving',\n",
       " 'cars.',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'technologies',\n",
       " 'like',\n",
       " 'machine',\n",
       " 'learning,',\n",
       " 'deep',\n",
       " 'learning,',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'data',\n",
       " 'improve',\n",
       " 'performance',\n",
       " 'time.',\n",
       " 'As',\n",
       " 'AI',\n",
       " 'continues',\n",
       " 'evolve,',\n",
       " 'holds',\n",
       " 'potential',\n",
       " 'greatly',\n",
       " 'transform',\n",
       " 'industries',\n",
       " 'healthcare,',\n",
       " 'education,',\n",
       " 'transportation,',\n",
       " 'finance,',\n",
       " 'also',\n",
       " 'raising',\n",
       " 'important',\n",
       " 'questions',\n",
       " 'ethics,',\n",
       " 'privacy,',\n",
       " 'job',\n",
       " 'displacement.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc43b7e-46b8-4118-a572-6c58ab18670f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2cddc-28d7-43bd-b510-5f3aebc04a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword hatayesi pani aako words haru hamro list mai chhaa if paragraph ma chaiye joinn garnu parchhaa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cf4b48b-3a67-4e77-bca3-117e59cec65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every word lai space ley connect garnu parchhca. \n",
    "final_content = ' '.join(after_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea48ee40-d01d-4ca9-a3c0-cfbe06401cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) branch computer science aims create machines capable performing tasks typically require human intelligence. These tasks include learning experience, understanding natural language, recognizing images patterns, making decisions. AI already integrated many aspects daily lives, virtual assistants, recommendation systems, facial recognition, self-driving cars. It uses technologies like machine learning, deep learning, natural language processing analyze large amounts data improve performance time. As AI continues evolve, holds potential greatly transform industries healthcare, education, transportation, finance, also raising important questions ethics, privacy, job displacement.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0f3260f-83f3-498e-9eaf-8339dd772bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial+Intelligence+(AI)+branch+computer+science+aims+create+machines+capable+performing+tasks+typically+require+human+intelligence.+These+tasks+include+learning+experience,+understanding+natural+language,+recognizing+images+patterns,+making+decisions.+AI+already+integrated+many+aspects+daily+lives,+virtual+assistants,+recommendation+systems,+facial+recognition,+self-driving+cars.+It+uses+technologies+like+machine+learning,+deep+learning,+natural+language+processing+analyze+large+amounts+data+improve+performance+time.+As+AI+continues+evolve,+holds+potential+greatly+transform+industries+healthcare,+education,+transportation,+finance,+also+raising+important+questions+ethics,+privacy,+job+displacement.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#suppose + le join garnaa man lagyooo\n",
    "final_content = '+'.join(after_stopwords)\n",
    "final_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e46ce-1899-4f57-a580-460821f63aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380d1ea-f2b0-46bb-a60f-86cadb017b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word visualize garnee case jasto ki kun words frequently used vako chhaa, yesto belaaa ta stopwords lai hatuanaa parchhcaa kinaki \n",
    "hataiyenaaa bhaney ta obviously yeho words hunchhan frequently repeated...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13b02dc2-ca32-4e4e-9a60-7159e50a6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num= [2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4f866da-f0ec-4db3-8b4d-6b13eb6cc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#square garna ko lagi\n",
    "sq_list = []\n",
    "for x in num:\n",
    "    sq = x ** 2\n",
    "    sq_list.append(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30e2512e-a34d-4906-9f6f-c3d37d926ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 9, 16, 25, 36]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c097a1-3185-43ac-b737-a51410c98d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  yo logic yek line mai garna sakenchaa with the help of list comprehension..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33159aef-668b-4a18-82f6-8602bc5c1399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list compreshension \n",
    "sq_list = [x**2 for x in num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df406aa0-3705-409e-8113-c9afb2fa5f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 9, 16, 25, 36]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e31e6e-e7db-48df-b823-d4b87101a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment \n",
    "\n",
    "\n",
    "same kuraa spacy bataa garnee(5 steps sammaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef5eb1-bcb3-4d50-9fbf-52089aff7d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af4d4484-520f-47f7-81ba-cbf434c7094a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472c06b-9e27-4f82-bf4b-ae0ccbdd57b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca1bfd56-23c7-4094-8bbe-3b4b9d3e84eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm \n",
    "#function implement gardenchhaa yeslee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed80c2-0b46-411b-8b97-5feae8f7c442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
